---
title: "Project I"
author: "Remco Bast"
date: "3-10-2020"
output: html_document
---

# Project - Het voorspellen van zorgkosten

De dataset was verkregen van kaggle: https://www.kaggle.com/mirichoi0218/insurance en de data zou ook beschikbaar zijn bij het kopen van het boek Machine Learning With R van Brett Lantz. 

Het doel van de dataset is om zo goed mogelijk de "charges", ofwel de zorgkosten van individuen zo goed mogelijk te voorspelen. Het betreft een dataset met 1338 observaties en 7 variabelen. 

## Inladen van libraries en de data

```{r, warning = FALSE}
#suppress om start up message te voorkomen
sm <- suppressMessages

sm(library(tidyverse))
sm(library(ggplot2))
sm(library(corrplot))
sm(library(scales))
sm(library(caret))
sm(library(Metrics))
```

```{r}
data <- read.csv("https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv")
```

## Data exploratie

Nu de data is ingeladen laten we eens kijken naar wat voor variabelen we hebben en hoe deze eruit zien.
```{r}
head(data)
```
We hebben dus informatie over de leeftijd, geslacht, aantal kinderen, bmi, rookstatus, de regio, en tot slot de uitkomst genaamd charges.  

Laten we beginnen met het kijken naar descriptive statistics
```{r}
summary(data)
```

Hieruit blijkt al snel dat de personen in deze dataset tussen de 18 en 64 jaar oud zijn, dat er ongeveer even veel vrouwen als mannen in de dataset zitten en dat deze personen goed verdeeld zijn over de verschillende regio's. Qua BMI zien we dat het gemiddelde BMI rond de 30 ligt, wat er dus al op duidt dat de gemiddelde persoon in deze dataset overgewicht heeft. Verder zien we dat de meeste mensen in deze dataset niet roken (+- 93%) en dat het aantal kinderen tussen de 0 en de 5 ligt. De uitkomstmaat charges ligt tussen de 1122 en 63770 en aangezien het een dataset uit Amerika is ga ik er vanuit dat de charges in USD zijn. De gemiddelde zorgkosten zijn in deze dataset 13270 USD. 

We kunnen ook gelijk even kijken naar de correlaties tussen de numerieke variabelen:

```{r}
data %>%
  select_if(is.numeric) %>%
  cor()
```
Hierbij zien we dat er kleine correlaties zijn tussen leeftijd en zorgkosten (0.3) en bmi en zorgkosten (0.2). 

```{r}
col <- colorRampPalette(c("#50A3EB", "#06233D"))

data %>%
  select_if(is.numeric) %>%
  cor() %>%
  corrplot(method = "number", type = "upper", tl.col = "black", tl.srt = 45, 
           col = col(200))

# https://www.webfx.com/web-design/color-picker/06233D  -> kleuren hexcodes
# http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram -> documentatie corrplot functie
```


Laten we nu eerst kijken naar de verdeling van de zorgkosten en vervolgens hoe de andere variabelen zich verhouden tot deze zorgkosten.

Visualisatie verdeling zorgkosten:
```{r}
ggplot(data = data, mapping = aes(x = charges)) + 
  geom_histogram(bins = 35, fill = "lightblue") +
  labs(title = "Verdeling zorgkosten", y = "Aantal", x = "Zorgkosten (in USD)") +
  theme_light()
```

De meeste mensen hebben zorgkosten tussen de 1000 en +- 15000 USD. Een kleine groep mensen hebben zorgkosten van 20000-40000 USD en een enkeling heeft meer dan 50000 USD als zorgkosten.

Relatie tussen leeftijd en zorgkosten:
```{r}
ggplot(data = data, mapping = aes(x = age, y = charges)) + 
  geom_boxplot(aes(group = age, fill = age)) + 
  theme_light() +
  labs(title = "Zorgkosten op basis van leeftijd", x = "Leeftijd", y = "Zorgkosten (in USD)")
```

Er is een kleine relatie te zien tussen leeftijd en zorgkosten; naarmate de leeftijd toeneemt nemen ook de zorgkosten ook toe.

Zorgkosten voor mannen en vrouwen:
```{r}
ggplot(data = data, mapping = aes(x = sex, y = charges, fill = sex)) + 
  geom_boxplot() + 
  theme_light() +
  labs(title = "Zorgkosten op basis van geslacht", y = "Zorgkosten (in USD)", x = "Geslacht")
```

Er zit niet veel verschil tussen de zorgkosten van mannen en vrouwen in deze dataset.

Zorgkosten en roken:
```{r}
ggplot(data = data, mapping = aes(x = smoker, y = charges, fill = smoker)) + 
  geom_boxplot() + 
  theme_light() +
  labs(title = "Zorgkosten op basis van roken", y = "Zorgkosten (in USD)", x = "Roken")
```

Wel zit er een groot verschil tussen de zorgkosten van rokers en niet-rokers. De mediane zorgkosten van rokers liggen ongeveer op de 35000 USD, terwijl die van niet-rokers maar rond de 8000 USD liggen.


Zorgkosten en regio:
```{r}
ggplot(data = data, mapping = aes(x = region, y = charges, fill = region)) + 
  geom_boxplot() + 
  theme_light() +
  labs(title = "Zorgkosten op basis van regio", y = "Zorgkosten (in USD)", x = "Regio")
```

Het ziet er naar uit dat de mensen uit de regio Noord-oost meer zorgkosten hebben dan mensen uit andere regio's. We kunnen dit ook numeriek bekijken door per regio te kijken naar de gemiddelde en mediane zorgkosten:

```{r}
data %>%
  group_by(region) %>%
  summarise(gemiddelde_zorgkosten = mean(charges), mediaan_zorgkosten = median(charges))
```

Daaruit blijkt dat de mensen uit de regio Zuid-Oost gemiddeld gezien de meeste zorgkosten hebben (+- 1000 - 2000 USD meer). Als we dan kijken naar de mediane zorgkosten zien we dat deze in de regio Noord-Oost het hoogst zijn gevolgd door de regio Zuid-Oost. Over het algemeen kunnen we dus zeggen dat de mensen uit het Oosten meer zorgkosten hebben dan mensen uit het Westen. 


Relatie tussen het aantal kinderen en zorgkosten:
```{r}
ggplot(data = data, mapping = aes(x = factor(children), y = charges, fill = factor(children))) + 
  geom_boxplot() + 
  theme_light() +
  labs(title = "Zorgkosten op basis van het aantal kinderen", y = "Zorgkosten (in USD)", x = "Aantal kinderen")
```

Het lijkt alsof mensen met 0 kinderen hogere zorgkosten hebben, en verder lijkt het alsof de zorgkosten oplopen naarmate iemand meer kinderen heeft vanaf het eerste kind (afgezien van 5 kinderen). Aangezien er hier geen ordinaliteit is, veranderen we deze variabele naar een factor.
```{r}
data$children <- factor(data$children)
```


Relatie tussen BMI en zorgkosten:
```{r}
ggplot(data = data, mapping = aes(x = bmi, y = charges)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_light() +
  labs(title = "Zorgkosten op basis van BMI", y = "Zorgkosten (in USD)", x = "BMI")
```

Naarmate de BMI toeneemt, nemen ook de zorgkosten toe. Wellicht is dit nog anders voor rokers en niet-rokers, dus laten we dit ook visualiseren:

```{r}
ggplot(data = data, mapping = aes(x = bmi, y = charges, fill = smoker)) + 
  geom_point(aes(color = smoker)) + 
  geom_smooth(method = "lm") +
  theme_light() +
  labs(title = "Zorgkosten op basis van BMI", y = "Zorgkosten (in USD)", x = "BMI")
```

We zien hierbij overduidelijk dat de relatie tussen BMI en zorgkosten voornamelijk toeneemt voor rokers.

Voor nu hebben we BMI continue gelaten, maar we kunnen deze BMI variabele ook veranderen naar een binaire variabele om aan te geven of iemand obesitas heeft (BMI > 30) of niet.
```{r}
data <- data %>%
  mutate(obesitas = bmi > 30)
```

Als we dan kijken naar de eerste paar observaties dan zien we dat we een nieuwe kolom hebben gemaakt genaamd obesitas en dat deze TRUE is als de BMI boven de 30 is en FALSE is als de BMI onder de 30 is.
```{r}
head(data)
```

Deze nieuwe kolom is nu een logical data type, maar als we deze willen meenemen in onze statistische analyses moeten we deze veranderen naar een factor variabele.
```{r}
data$obesitas <- factor(data$obesitas)
```

Laten we kijken of de zorgkosten verschillen tussen mensen met en zonder obesitas.
```{r}
ggplot(data = data, mapping = aes(x = obesitas, y = charges, fill = obesitas)) + 
  geom_boxplot() + 
  stat_summary(fun=mean, geom="point", shape=20, size=7, color="black") +
  labs(title = "Zorgkosten geen obesitas / wel obesitas", x = "Obesitas nee / ja", y = "Zorgkosten (in USD)") + 
  theme_light()
```

Hierbij zien we dat de mediane zorgkosten niet veel van elkaar verschillen of mensen wel of geen obesitas hebben. Echter, als we dan naar de gemiddelde zorgkosten kijken (aangegeven met de zwarte cirkel) dan zien we dat de mensen die wel obesitas hebben +- 5000 USD meer hebben aan zorgkosten.

Als we exact willen weten hoeveel verschil er zit tussen de gemiddelde zorgkosten van mensen en zonder obesitas dan kunnen we dit ook weer numeriek bekijken:
```{r}
data %>%
  group_by(obesitas) %>%
  summarise(gemiddelde_zorgkosten = mean(charges))
```


## Data opsplitsen in een training-set en test-set

```{r}
set.seed(101)

trainIndex <- createDataPartition(data$charges, p = .8, 
                                  list = FALSE, 
                                  times = 1)

train <- data[trainIndex, ]
test <- data[-trainIndex, ]
```

## Algoritmes toepassen

Machine learning algoritmes:

- Lineaire regressie
- Lasso regressie
- Random forest regressie 
- Knn regressie

### Lineaire regressie
```{r}
linregressie <- lm(charges ~ age + sex + children + region + obesitas + smoker, data = train)
summary(linregressie)
```

Geslacht lijkt geen verschil te maken in de lineaire regressie (P = > 0.05). Deze kunnen we dus verwijderen.

```{r}
linregressie2 <- lm(charges ~ age + children + region + obesitas + smoker, data = train)
summary(linregressie2)  
```

Verder zijn de regiocoÃ«fficienten ook niet significant, dus deze kunnen we er ook uit halen.

```{r}
linregressie3 <- lm(charges ~ age + children + obesitas + smoker, data = train)
summary(linregressie3)
```

Trainingsoptie instellen als repeated cross validation.
```{r}
trainoptie <- trainControl(method = "repeatedcv", number = 10, repeats = 5) 
```


```{r, cache = TRUE}
set.seed(1)
linregressie.fit <- train(charges ~ age + children + obesitas + smoker, 
                          data = train,
                          method = "lm",
                          trControl = trainoptie)

linregressie.fit
```

### Random forest
```{r, cache = TRUE}
set.seed(1)
randomforest.fit <- train(charges ~ age + sex + children + region + obesitas + smoker,
                          data = train,
                          method = "rf", # random forest 
                          trControl = trainoptie,
                          tuneGrid = expand.grid(mtry = c(1,2,3,4,5)), #mtry is hoeveel variabelen er geselecteerd worden voor de trees.
                          importance = TRUE)

randomforest.fit
varImp(randomforest.fit)
```

### K-nearest neighbors regressie
```{r, cache = TRUE}
set.seed(1)
knn.fit <- train(charges ~ age + sex + children + region + obesitas + smoker,
                 data = train,
                 preProcess = c("center", "scale"),
                 method = "knn", #knn 
                 trControl = trainoptie,
                 tuneLength = 25)

knn.fit

varImp(knn.fit)
```

### Lasso regressie
```{r, cache = TRUE}
set.seed(1)
lasso.fit <- train(charges ~ age + sex + children + region + obesitas + smoker, 
                   data = train, 
                   method = "glmnet", 
                   tuneGrid = expand.grid(
                     .alpha = 1,
                     .lambda = seq(0, 150, by = 3)),
                   trControl = trainoptie)

lasso.fit
varImp(lasso.fit)
```


Resultaten bij elkaar voegen
```{r}
resultaat <- resamples(list(KNN_regressie = knn.fit, Lasso_regressie = lasso.fit, Random_forest = randomforest.fit, Lineaire_regressie = linregressie.fit))
```

```{r}
dotplot(resultaat, main = "Training Resultaten (MAE, RMSE, Rsquared)")
```

Aangezien Random Forest de laagste Mean Absolute error en Root mean squared error heeft gebruiken we deze om predicties te maken op de  test dataset.

### Predicties maken
```{r}
predicties <- predict(randomforest.fit, newdata = test)
```


```{r}
rmse(test$charges, predicties)
```
De Root mean squared error van onze predicties op de test set is 4200, wat betekend dat de random forest algoritme het best goed doet op data die het niet eerder gezien had. 

